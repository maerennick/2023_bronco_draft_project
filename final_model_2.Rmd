---
title: "final_model_2"
author: "Mae Rennick"
date: "2023-08-15"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(randomForest)
library(tidyverse)
library(janitor)
library(nflreadr)
library(mice)
library(cluster)
library(dendextend)
library(kableExtra)
library(fmsb)
library(corrplot)
library(GGally)

```



## Player Production 

The amount of times a player is on the field is important to see how much they play. Offensive percentage determines the percent of offensive snaps taken. We assume that a higher offensive percentage is demonstrative of player production. 

In other analyses: performance criteria include several variables including: draft order; 3 years each of salary received and games played; and position-specific data; stats from the preceding year (college stats).

This investigation focuses on the correlation between athletic attributes of players and overall production (offensive percentage). Notably, previous studies have found inconsistent evidence to link combine tests and professional football performance (Kuzmits and Adams 2008) which is likely because athleticism is only one aspect of the game. However, by using athletic attributes as predictor variables in a model in which performance outcomes are known, we may be able to discern collective attributes that characterize players who perform well. 


## Project Components

1. Data Import, Cleaning and Exploration 

2. Data Imputation: Multiple Imputation

3. Random Forest Model

4. Cluster Analysis


### Data Import, Cleaning and Exploration

#### Import

```{r}
nfl_combine <- nflreadr::load_combine()
str(nfl_combine,  give.attr = F)

nfl_snap_counts <- nflreadr::load_snap_counts(seasons = 2012:2022) %>% 
  filter(position== "WR") %>% 
  filter(pfr_player_id!="NA") %>% 
  rename(pfr_id = pfr_player_id)
```

#### Cleaning

```{r}
convert_height_to_decimal <- function(ht) {
  components <- strsplit(ht, "-")[[1]]
  feet <- as.numeric(components[1])
  inches <- as.numeric(components[2])
  decimal_feet <- feet + inches / 12
  return(decimal_feet)
}

nfl_combine$decimal_height <- sapply(nfl_combine$ht, convert_height_to_decimal)

features <- c("pos", "decimal_height", "wt", "forty", "vertical", "bench", "broad_jump", "cone", "shuttle", "season", "pfr_id") ## relevant features for model training

wr_combine_data <- nfl_combine %>%
  select(features) %>%  ## only include relevant features
  rename(ht= decimal_height) %>% 
  filter(pos== "WR") %>% 
  filter(pfr_id!="NA")

nfl_pct_offense_avg_season<- nfl_snap_counts %>% 
  group_by(pfr_id, season) %>% 
  summarise(avg_pct_offense = mean(offense_pct)) 

nfl_pct_offense_avg<- nfl_snap_counts %>% 
  group_by(pfr_id) %>% 
  summarise(season= mean(season), avg_pct_offense = mean(offense_pct)) 


merged_all <- left_join(wr_combine_data, nfl_snap_counts, by = "pfr_id") ## note: dropped 7419 rows containing pfr_ids that were not in the combine dataset


merged_avg_season<- left_join(wr_combine_data, nfl_pct_offense_avg_season, by = "pfr_id") %>% 
  arrange(pfr_id, season.y) %>%  # Sort the data within each group
  group_by(pfr_id) %>%
  mutate(
    season_rank = dense_rank(desc(season.y)),  # Assign ranks to seasons
    season_number = case_when(
      is.na(season.y) ~ "1",  # Handle NA values
      season_rank == 1 ~ "1",
      season_rank == 2 ~ "2",
      TRUE ~ paste0(season_rank)
    )
  ) %>%
  ungroup() %>%
  select(-season_rank)   # Remove the temporary rank column


merged_avg<- left_join(wr_combine_data, nfl_pct_offense_avg, by = "pfr_id")

```


#### Exploration

1. On average, how does player production change throughout their career (by season)?

```{r}

ggplot(merged_avg_season, aes(x=as.numeric(season_number), y= avg_pct_offense))+
  geom_point(color ="#002144")+
  geom_smooth(color = "#FA4E13")+
  theme_minimal()

### visually: it looks like players who make it to their 7th+ season are more likely to have a higher offensive_pct. This may influence model outcomes 


```

2. What does average player production look like in the first season? 

```{r}

first_season<- merged_avg_season %>% 
  filter(season_number== 1)

ggplot(merged_avg, aes(x=avg_pct_offense))+
  geom_histogram(fill = "#002144")+
  theme_minimal()

## quite a few players getting a lot of playing time in their first season

```


3. What does average player production look like average across all seasons by player?

```{r}

combined_seasons<- merged_avg_season %>% 
  group_by(pfr_id) %>% 
  summarise(avg_pct_offense = mean(avg_pct_offense))

ggplot(combined_seasons, aes(x=avg_pct_offense))+
  geom_histogram(fill = "#FA4E13")+
  theme_minimal()

## normal-ish distribution with a slight right skew

```


4. 2023 combine class 

```{r}

merged_avg_2023<- merged_avg %>% 
  filter(season.x == 2023)

summary <- merged_avg_2023 %>%
  filter(!is.na(ht) & !is.na(wt) & !is.na(forty) & !is.na(vertical) &
           !is.na(bench) & !is.na(broad_jump) & !is.na(cone) & !is.na(shuttle)) %>%
  summarise(total_rows = n())


# Create a function to count non-NA values in a column
count_non_na <- function(column) {
  sum(!is.na(column))
}

# Count non-NA values in each column
column_counts <- sapply(merged_avg_2023, count_non_na)

# Create a data frame for plotting
count_df <- data.frame(Column = names(column_counts), Count = column_counts)

ggplot(count_df, aes(x = Column, y = Count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Columns", y = "Count", title = "Non-NA Value Counts in Columns") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



# Count non-NA values in each column
column_counts_all <- sapply(merged_avg, count_non_na)

# Create a data frame for plotting
count_df_all <- data.frame(Column = names(column_counts_all), Count = column_counts_all)

ggplot(count_df_all, aes(x = Column, y = Count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Columns", y = "Count", title = "Non-NA Value Counts in Columns") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```


### Data Imputation: Multiple Imputation 

Missing Values: Only two players from the 2023 combine competed in all events.

For random forest, it's important to choose an imputation method that preserves the relationships and patterns in the data as closely as possible. Among the options, "Multiple Imputation" is often recommended to minimize bias in the context of building predictive models like random forests.

Multiple Imputation: Generate multiple imputed datasets, build models on each, and combine the results.
- moderate to high amount of missing data.
- missing data mechanism is not ignorable (MAR or MNAR).


#### Non-imputed Data


```{r}

data<- merged_avg %>%
  #select(-bench, -cone, -shuttle) %>% 
  filter(rowSums(!is.na(select(., forty, vertical, bench, broad_jump, cone, shuttle))) >= 3) %>%   ## must have values for at least three events
  filter(
    !is.na(avg_pct_offense) |
    (is.na(avg_pct_offense) & season.x == 2023)
  ) ## only include predictive values for players who have played in the league (except 2023)

non_imputed_data<- data %>% 
  drop_na() %>% 
  select(ht, wt, forty, vertical, cone, shuttle, avg_pct_offense)

```

#### Imputed Data and Verification

```{r}

# Count non-NA values in each column
column_counts_all_2 <- sapply(data, count_non_na)

# Create a data frame for plotting
count_df_all_2 <- data.frame(Column = names(column_counts_all_2), Count = column_counts_all_2)

ggplot(count_df_all_2, aes(x = Column, y = Count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Columns", y = "Count", title = "Non-NA Value Counts in Columns") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Set the number of imputations
num_imputations <- 10

# Create an imputation object using the 'mice' function
imputed_data <- mice(data[, c("pfr_id", "season.x", "ht", "wt", "forty", "bench", "vertical", "broad_jump", "cone", "shuttle")], m = num_imputations)

# Perform the imputation
imputed_data <- complete(imputed_data)

## Evaluate imputation 

# Calculate missing data proportions for each variable
missing_proportions <- colMeans(is.na(data))

# Calculate correlations (only for numeric columns)
numeric_data <- data[, sapply(data, is.numeric)] %>% 
  drop_na() %>% ## get rid of NA values for comparison
  select(season.x, ht, wt, forty, bench, vertical, broad_jump, cone, shuttle)

correlation_original <- cor(numeric_data, use = "complete.obs")
correlation_imputed <- cor(imputed_data[, sapply(imputed_data, is.numeric)])

# Plot correlations
corrplot(correlation_original, method = "color", title = "Correlation Matrix (Original Data)")
corrplot(correlation_imputed, method = "color", title = "Correlation Matrix (Imputed Data)")

### result: slight shift, but overall similar correlations

#Compare means, medians, and standard deviations
original_summary <- apply(numeric_data, 2, function(x) c(mean(x), median(x), sd(x)))
imputed_summary <- apply(imputed_data[, sapply(imputed_data, is.numeric)], 2, function(x) c(mean(x), median(x), sd(x)))

## turn values into a dataframe for visulatization
summary_df <- data.frame(
  Variable = rep(names(numeric_data), each = 3),
  Measure = rep(c("Mean", "Median", "Standard Deviation"), times = ncol(numeric_data)),
  Value = c(original_summary, imputed_summary),
  Data = rep(c("Original", "Imputed"), each = length(names(numeric_data)) * 3)
)

ggplot(summary_df, aes(x = Variable, y = Value, fill = Measure)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_grid(Data ~ Measure, scales = "free_y", switch = "y") +
  labs(title = "Comparison of Means, Medians, and Standard Deviations",
       y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```


### Random Forest Model

#### Predictor Variable selection

1. Selecting variables 

According to Meil et al. 2018, success in the NFL were significantly correlated (p < .05) with several physical ability measures:
 - player size (ht and wt)
 - forty-yard dash (forty)
 - vertical jump height (vertical)
 - twenty-yard shuttle (shuttle)
 - 3-cone drill(cone)
 
 However, the measure of player success was not offensive percentage. 
 
 
2. Handling correlated/nested variables

Height and weight are highly correlated in this dataset, therefore I will use a combined BMI score to account for both. 

#### Model Building

##### With Imputation

```{r}
## Bind imputed data with snap data 

merged_avg_imputed<- left_join(imputed_data, nfl_pct_offense_avg, by = "pfr_id") %>%
  filter(season.x != 2023) %>% # do not include 2023 players in model
  filter(avg_pct_offense != "NA") %>% 
  select(ht, wt, forty, bench, vertical, broad_jump, cone, shuttle, avg_pct_offense) %>% 
  mutate(ht_in= ht*12) %>%  # convert feet to inches for BMI calc
  mutate(BMI = wt/(ht_in^2)*703) %>%  #The multiplication by 703 is used to convert the BMI calculation to the standard units used in the BMI formula.
  select(-ht_in, -ht, -wt)

## Split the data into training and testing sets

set.seed(18)  # for reproducibility
train_indices <- sample(1:nrow(merged_avg_imputed), 0.7 * nrow(merged_avg_imputed))
train_data <- merged_avg_imputed[train_indices, ]
test_data <- merged_avg_imputed[-train_indices, ]

## Train the Random Forest model

rf_model <- randomForest(avg_pct_offense ~ ., data = train_data, ntree = 500)

rf_model

## Test the model 

test_predictions <- predict(rf_model, newdata = test_data)

# Calculate the Mean Squared Error (MSE) as a measure of model performance
mse <- mean((test_data$avg_pct_offense - test_predictions)^2)
print(paste("Mean Squared Error on Test Data:", mse)) 

# Variable importance scores
variable_importance <- importance(rf_model)
print(variable_importance)

variable_importance_df <- data.frame(
  variable = row.names(variable_importance),
  importance = as.vector(variable_importance)  # Convert to a vector if necessary
)

ggplot(variable_importance_df, aes(x = reorder(variable, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Variable Importance",
       x = "Variable",
       y = "Importance") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


##### Without Imputation

```{r}

## Split the data into training and testing sets

set.seed(18)  # for reproducibility
train_indices_2 <- sample(1:nrow(non_imputed_data), 0.7 * nrow(non_imputed_data))
train_data_2 <- non_imputed_data[train_indices_2, ]
test_data_2 <- non_imputed_data[-train_indices_2, ]

## Train the Random Forest model

rf_model_2 <- randomForest(avg_pct_offense ~ ., data =train_data_2, ntree = 500)

rf_model_2

## Test the model 

test_predictions_2 <- predict(rf_model_2, newdata = test_data_2)

# Calculate the Mean Squared Error (MSE) as a measure of model performance
mse <- mean((test_data_2$avg_pct_offense - test_predictions_2)^2)
print(paste("Mean Squared Error on Test Data:", mse)) 

```


#### Model Verification

```{r}
## sensitivity analysis

## Number of trees 

ntree_values <- c(100, 200, 300, 400, 500)
mse_values <- numeric(length(ntree_values))

for (i in seq_along(ntree_values)) {
  rf_model <- randomForest(avg_pct_offense ~ ., data = train_data, ntree = ntree_values[i])
  test_predictions <- predict(rf_model, newdata = test_data)
  mse_values[i] <- mean((test_data$avg_pct_offense - test_predictions)^2)
}

plot(ntree_values, mse_values, type = "b", xlab = "Number of Trees", ylab = "Mean Squared Error")


## variables considered at each split

mtry_values <- seq(1, ncol(train_data) - 1)
mse_values_mtry <- numeric(length(mtry_values))

for (i in seq_along(mtry_values)) {
  rf_model <- randomForest(avg_pct_offense ~ ., data = train_data, ntree = 500, mtry = mtry_values[i])
  test_predictions <- predict(rf_model, newdata = test_data)
  mse_values_mtry[i] <- mean((test_data$avg_pct_offense - test_predictions)^2)
}

plot(mtry_values, mse_values_mtry, type = "b", xlab = "Number of Variables at Each Split (mtry)", ylab = "Mean Squared Error")

```


#### Model Results: Player Ranking

```{r}
## Make a dataset with only 2023 combine class
imputed_2023_merged<- left_join(imputed_data, nfl_pct_offense_avg, by = "pfr_id") %>% 
  arrange(pfr_id, season.x) %>%  # Sort the data within each group
  group_by(pfr_id) %>%
  mutate(
    season_rank = dense_rank(desc(season.x)),  # Assign ranks to seasons
    season_number = case_when(
      is.na(season) ~ "1",  # Handle NA values
      season_rank == 1 ~ "1",
      season_rank == 2 ~ "2",
      TRUE ~ paste0(season_rank)
    )
  ) %>%
  ungroup() %>%
  select(-season_rank) %>%    # Remove the temporary rank column
  filter(season.x == 2023) %>%
  mutate(ht_in= ht*12) %>%  # convert feet to inches for BMI calc
  mutate(BMI = wt/(ht_in^2)*703) %>%  #The multiplication by 703 is used to convert the BMI calculation to the standard units used in the BMI formula.
  select(BMI, forty, broad_jump, vertical, cone, bench, shuttle, avg_pct_offense)

## Make predictions using the trained model
predictions_2023 <- predict(rf_model, newdata = imputed_2023_merged)


## Rank the players

imputed_2023_merged_id<- left_join(imputed_data, nfl_pct_offense_avg, by = "pfr_id") %>% 
  arrange(pfr_id, season) %>%  # Sort the data within each group
  group_by(pfr_id) %>%
  mutate(
    season_rank = dense_rank(desc(season)),  # Assign ranks to seasons
    season_number = case_when(
      is.na(season) ~ "1",  # Handle NA values
      season_rank == 1 ~ "1",
      season_rank == 2 ~ "2",
      TRUE ~ paste0(season_rank)
    )
  ) %>%
  ungroup() %>%
  select(-season_rank) %>%    # Remove the temporary rank column
  filter(season.x == 2023) %>% 
    mutate(ht_in= ht*12) %>%  # convert feet to inches for BMI calc
  mutate(BMI = wt/(ht_in^2)*703) %>%
  select(BMI, forty, vertical, cone, shuttle, avg_pct_offense, pfr_id)

ranked_2023 <- cbind(imputed_2023_merged_id, predictions_2023)
ranked_2023 <- ranked_2023 %>% 
  arrange(desc(predictions_2023)) %>% 
  select(pfr_id, predictions_2023)

nfl_combine_names<- nfl_combine %>% 
  select(pfr_id, player_name) %>% 
  filter(pfr_id != "NA")

name_ranks <- left_join(nfl_combine_names, ranked_2023, by = "pfr_id") %>% 
  filter(predictions_2023 != "NA") %>% 
  arrange(desc(predictions_2023))
  
print(name_ranks)
```


#### Ranking based on average comparison 


```{r}

# Calculate the average across all columns and rows
averages <- rowMeans(imputed_data[, -(1:2)])  # Exclude the non-numeric column

# Calculate the absolute differences between each value and the corresponding row's average
diff_from_avg <- imputed_data %>%
  mutate(across(-(1:2), ~ abs(. - averages)))  # Exclude the non-numeric column

# Summarize the differences for each row
row_diff_summary <- diff_from_avg %>%
  mutate(row_diff_sum = rowSums(across(-c(pfr_id, season.x))))

# Rank the rows based on the summed differences
ranked_rows <- row_diff_summary %>%
  arrange(row_diff_sum) %>%
  mutate(rank = row_number())

ranked_rows_2023<- left_join(ranked_rows, name_ranks, by = "pfr_id") %>% 
  filter(season.x ==2023) %>% 
  group_by(pfr_id, player_name, rank, predictions_2023) %>% 
  summarise()

```



### Cluster Analysis

Cluster analysis is a technique used to group similar data points together based on their characteristics. The goal is to identify patterns, relationships, or structures within the data.

```{r}
# Select relevant columns for clustering
selected_columns <- c('BMI', 'vertical', 'bench', 'broad_jump', 'forty', 'cone', 'shuttle')

imputed_data_BMI<- imputed_data %>% 
  mutate(ht_in= ht*12) %>%  # convert feet to inches for BMI calc
  mutate(BMI = wt/(ht_in^2)*703)
  

# Subset the dataframe
subset_data <- imputed_data_BMI[selected_columns] 

# Perform hierarchical clustering
dist_matrix <- dist(subset_data)
hierarchical_clusters <- hclust(dist_matrix)
num_clusters <- 3 

# Cut the dendrogram to get cluster assignments
cluster_assignments <- cutree(hierarchical_clusters, k = num_clusters)

# Add the cluster assignments to the original dataframe
data_with_clusters <- imputed_data %>%
  mutate(cluster = cluster_assignments)

data_with_clusters_names<- left_join(nfl_combine_names, data_with_clusters, by = "pfr_id")%>% 
  filter(season.x != "NA")

# Determine optimal number of clusters using silhouette score
silhouette_scores <- numeric(10)  # Try up to 10 clusters
for (i in 2:10) {
  cluster_assignments <- cutree(hierarchical_clusters, k = i)
  silhouette_obj <- silhouette(cluster_assignments, dist_matrix)
  silhouette_scores[i] <- mean(silhouette_obj[, "sil_width"])
}
optimal_num_clusters <- which.max(silhouette_scores)

# Calculate the average silhouette width for the optimal number of clusters
optimal_cluster_assignments <- cutree(hierarchical_clusters, k = optimal_num_clusters)
optimal_silhouette_obj <- silhouette(optimal_cluster_assignments, dist_matrix)
optimal_silhouette_avg <- mean(optimal_silhouette_obj[, "sil_width"])

# Print results
cat("Optimal number of clusters:", optimal_num_clusters, "\n")
cat("Average silhouette width for optimal clusters:", optimal_silhouette_avg, "\n")


#A silhouette score of 0.25537 is a positive value, which means the clusters in are somewhat well-separated and distinguishable, but there might be some overlap or proximity between neighboring clusters. 

```



```{r}

## Current Bronco WRs= Jerry Jeudy, Courtland Sutton, Mrquez Callaway, Lil' Jordan Humphrey, Montrell Washington (no combine stats for him), Kendall Hinton (no data on him either)
# 	JeudJe00, SuttCo00, CallMa01, HumpLi01


current_broncos_clusters<- data_with_clusters %>% 
  filter(pfr_id == "JeudJe00" | pfr_id == "SuttCo00" | pfr_id == "CallMa01" | pfr_id == "HumpLi01") %>% 
  group_by(pfr_id) %>% 
  summarise(mean(season.x), mean(BMI),mean(forty),mean(vertical), mean(cone),mean(shuttle), mean(cluster))


clusters_2023_class<- data_with_clusters_names %>% 
  filter(season.x == 2023)

# Plot the dendrogram
dend <- as.dendrogram(hierarchical_clusters)
#dend %>% plot(horiz = TRUE)

# visual exploration of clusters
selected_vars <- c('ht', 'shuttle')
selected_data <- data_with_clusters %>%
  select(pfr_id, cluster, all_of(selected_vars))

ggplot(selected_data, aes(x = BMI, y = shuttle, color = factor(cluster))) +
  geom_point() +
  labs(x = "Height", y = "Shuttle", color = "Cluster") +
  theme_minimal()


# Filter data for the 2023 class
class_2023 <- data_with_clusters_names %>%
  filter(season.x == 2023)  # Adjust column name here

# Prepare the data for the radar chart
radar_data <- class_2023 %>%
  select(-pfr_id, -season.x, -player_name)

# Normalize the data to a [0, 1] scale (required for radar chart)
normalized_data <- scale(radar_data)

# Add column names to the normalized data
colnames(normalized_data) <- colnames(radar_data)

# Convert the data to a dataframe
normalized_df <- as.data.frame(normalized_data)

radar_chart <- ggplot(normalized_df, aes(group = cluster)) +
  geom_polygon(aes(x = as.numeric(row.names(normalized_df)), y = normalized_df[,1]), color = "black", size = 1, alpha = 0.5) +
  geom_text(data = class_2023, aes(x = as.numeric(row.names(normalized_df)), y = normalized_df[,1], label = player_name), hjust = 0) +
  coord_polar() +
  theme_minimal() +
  theme(axis.text.x = element_blank(), axis.title.x = element_blank()) +
  labs(fill = "Cluster")

# Display the radar chart
print(radar_chart)

### all of the clusters are on top of each other in a radar chart, suggesting that the average values of the characteristics for each cluster are very similar across all variables.

# Group by cluster and calculate average characteristics
cluster_avg_characteristics <- data_with_clusters %>%
  group_by(cluster) %>%
  summarize(
    avg_BMI = mean(BMI),
    avg_vertical = mean(vertical),
    avg_forty = mean(forty),
    avg_cone = mean(cone),
    avg_shuttle = mean(shuttle)
  )

cluster_table <- cluster_avg_characteristics %>%
  mutate_at(vars(starts_with("avg_")), round, digits = 2) %>%
  kable() %>%
  kable_styling(
    full_width = FALSE,
    bootstrap_options = c("striped", "hover", "condensed", "responsive")
  )

print(cluster_table)


```


